{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "file = get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import strings\n",
    "\n",
    "text = open(file, 'rb').read().decode(encoding='UTF-8')\n",
    "vocabulary = list(sorted(set(text)))\n",
    "chars = strings.unicode_split(text, input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=string, numpy=array([b'F', b'i', b'r', ..., b'g', b'.', b'\\n'], dtype=object)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "ids_from_chars = StringLookup(vocabulary=vocabulary, mask_token=None)\n",
    "ids = ids_from_chars(chars)\n",
    "chars_from_ids = StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars = chars_from_ids(ids)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "    _chars = chars_from_ids(ids)\n",
    "    return strings.reduce_join(_chars, axis=-1)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import data\n",
    "\n",
    "SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = SEQUENCE_LENGTH + 1\n",
    "EXAMPLES_PER_EPOCH = len(text)//BATCH_SIZE\n",
    "\n",
    "def split_dataset(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    output_text = sequence[1:]\n",
    "    return input_text, output_text\n",
    "\n",
    "raw_dataset = data.Dataset.from_tensor_slices(ids)\n",
    "sequences = raw_dataset.batch(BATCH_SIZE, drop_remainder=True).map(split_dataset)\n",
    "datasets = sequences.shuffle(10_000).batch(64, drop_remainder=True).prefetch(data.AUTOTUNE)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        input: b'ot in the giving vein to-day.\\n\\nBUCKINGHAM:\\nWhy, then resolve me whether you will or no.\\n\\nKING RICHAR'\n",
      "        vs\n",
      "        output: b't in the giving vein to-day.\\n\\nBUCKINGHAM:\\nWhy, then resolve me whether you will or no.\\n\\nKING RICHARD'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for input_text, output_text in datasets.take(1):\n",
    "    print(f\"\"\"\n",
    "        input: {text_from_ids(input_text[0]).numpy()}\n",
    "        vs\n",
    "        output: {text_from_ids(output_text[0]).numpy()}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, vocabulary_size, embedding_dimension=256, rnn_units=1024):\n",
    "        super().__init__(self)\n",
    "        self.embedding = Embedding(vocabulary_size, embedding_dimension)\n",
    "        self.gru = GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = Dense(vocabulary_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = self.embedding(inputs, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "model = MyModel(len(ids_from_chars.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        expected: b\"n which doth control't.\\n\\nBRUTUS:\\nHas said enough.\\n\\nSICINIUS:\\nHas spoken like a traitor, and shall an\"\n",
      "        actual: b\"?H\\nfBkt;uwxVu:ttnzKlOC:s,iKoGAxrrT$XViKwCCQd CrUtyDpoe,LofAA'W DZ[UNK]MzpTk3bbpuzR!lBoDWFuJj[UNK]UNP.H&E:?oq\"\n",
      "        loss: 4.189743518829346 66.005859375\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow import random, squeeze, exp\n",
    "\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "for input_batch, expected_batch in datasets.take(1):\n",
    "    actual_output = model(input_batch)\n",
    "    example_loss = loss(expected_batch, actual_output)\n",
    "    actual_output = actual_output[0]\n",
    "    actual_output = random.categorical(actual_output, num_samples=1)\n",
    "    actual_output = squeeze(actual_output, axis=-1)\n",
    "    actual_output = text_from_ids(actual_output)\n",
    "    expected_output = text_from_ids(expected_batch[0])\n",
    "    print(f\"\"\"\n",
    "        expected: {expected_output}\n",
    "        actual: {actual_output}\n",
    "        loss: {example_loss} {exp(example_loss)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pathlib\n",
    "import tempfile\n",
    "\n",
    "checkpoint_root = pathlib.Path(tempfile.mkdtemp() + 'generator-1')\n",
    "checkpoint_prefix = str(checkpoint_root/'checkpoint-{epoch}')\n",
    "checkpoint_callback = ModelCheckpoint(checkpoint_prefix, monitor='val_loss', save_weights_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 5s 16ms/step - loss: 2.7203\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 4s 15ms/step - loss: 1.9947\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.7104\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.5465\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 4s 15ms/step - loss: 1.4465\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 4s 15ms/step - loss: 1.3771\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 4s 15ms/step - loss: 1.3233\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.2785\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.2374\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.1980\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.1578\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 4s 17ms/step - loss: 1.1172\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.0734\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 1.0275\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 0.9782\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 0.9294\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 4s 15ms/step - loss: 0.8766\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 4s 15ms/step - loss: 0.8255\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 4s 16ms/step - loss: 0.7737\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 4s 15ms/step - loss: 0.7257\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(datasets, epochs=20, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import SparseTensor, sparse, constant, argmax\n",
    "\n",
    "mask_ids = ids_from_chars(['[UNK]'])[:, None]\n",
    "mask = SparseTensor(mask_ids, [-float('inf')] * len(mask_ids), [len(ids_from_chars.get_vocabulary())])\n",
    "mask = sparse.to_dense(mask)\n",
    "\n",
    "def generate_one_step(inputs, states=None):\n",
    "    inputs = strings.unicode_split(inputs, input_encoding='UTF-8')\n",
    "    inputs = ids_from_chars(inputs).to_tensor()\n",
    "    predicted, states = model(inputs, states, return_state=True)\n",
    "    predicted = predicted[:, -1, :]\n",
    "    predicted = predicted + mask\n",
    "    predicted = random.categorical(predicted, num_samples=1)\n",
    "    predicted = squeeze(predicted, axis=-1)\n",
    "    predicted = chars_from_ids(predicted)\n",
    "\n",
    "    return predicted, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b\"ROMEO:\\nWhy do you remuire? You bring him truel and lig!\\n\\nJULIET:\\n'Tis but possible.\\n\\nDot him:\\nYour prate-pierce loving friends,' in this same perfect with holy\\nwhomat, you will purchase good.\\nWhy standing courts upon him; 'What bear almost\\nGiven in the windsworms slain here to thy daughter.\\nThen lups are to beat more prizent and spider'd\\nCrevill be out from me a hundred apes,\\nYour sleep doth quit it o' the subjects of my virthes;\\nAnd spun in love.\\n\\nRIVERS:\\nMadam, I will not do it; yet I'll give you:\\nI shall tell no thee from the sea, and his new gown,\\nMy fear's son is mine offressed with her behoved.\\nMy father Was get a stabbed to her,\\nAnd he shall bear them true death with the hallow,\\nWould say he-wounds, with any spirit comes;\\nHaste you found my ben, i' the chboited heart;\\nAnd even himself I teed thee deep trample;\\nHe cannot, but die with the officers,\\nAnd fetch the usuries act of heavier than a wall\\nO mosalties; open with him in that son-foolish'd heaven,\\nHave at the levies to your crown\"], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "next_char = constant(['ROMEO'])\n",
    "states = None\n",
    "result = [next_char]\n",
    "\n",
    "for i in range(1_000):\n",
    "    next_char, states = generate_one_step(next_char, states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(strings.join(result))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
